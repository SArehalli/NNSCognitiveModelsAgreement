{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d96f22",
   "metadata": {},
   "source": [
    "Trying a causual intervention strategy to determine how much the representations built for CCG supertagging are used in language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b523862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "model_path = \"../CCGMultitask/models/augment/augment_.50_0_sgd_continue\"\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54814f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../CCGMultitask/\")\n",
    "from model import MultiTaskModel\n",
    "from train_augment import evaluate_lm, evaluate_ccg\n",
    "from train_joint import evaluate_joint\n",
    "from data import joint_tag_lm, augment_tag_lm, AugmentDataset, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path + \".w2idx\", \"rb\") as w2idx_f:\n",
    "    w2idx = pickle.load(w2idx_f)\n",
    "    \n",
    "vocab = {i:w for (w, i) in w2idx.items()}\n",
    "    \n",
    "with open(model_path + \".c2idx\", \"rb\") as c2idx_f:\n",
    "    c2idx = pickle.load(c2idx_f)\n",
    "    \n",
    "categories = {i:c for (c, i) in c2idx.items()}\n",
    "    \n",
    "model = MultiTaskModel(len(w2idx.keys()), 650, 650, [len(w2idx.keys()), len(c2idx.keys())], 2)\n",
    "model.load_state_dict(torch.load(model_path + \".pt\", map_location = torch.device(\"cpu\")))\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "if cuda: model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = joint_tag_lm(\"../CCGMultitask/data/ccg_supertags/ccg.23.common\", \n",
    "                         \"../CCGMultitask/data/ccg_supertags/categories\", 35, w2idx=w2idx)\n",
    "test_sampler = BatchSampler(test_data, 10)\n",
    "test_loader = DataLoader(test_data, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d38fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([w2idx[w.lower()] for w in \"<eos> The key to the cabinet\".split()])\n",
    "\n",
    "if cuda: input = input.cuda()\n",
    "    \n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "out, hidden = model.lstm(input.view(-1, 1), hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b00a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s:\t-1.6922204494476318\n",
      "is:\t-1.7930352687835693\n",
      "was:\t-2.329763174057007\n",
      "of:\t-2.6500298976898193\n",
      ",:\t-3.3813416957855225\n",
      "---\n",
      "N:\t-0.01167889591306448\n",
      "N/N:\t-4.456457138061523\n",
      "N/S[em]:\t-11.797123908996582\n",
      "(S[dcl]\\NP)/(S[b]\\NP):\t-18.4254207611084\n",
      "NP\\NP:\t-18.518659591674805\n"
     ]
    }
   ],
   "source": [
    "state = out.squeeze()[-1]\n",
    "\n",
    "# Generate outputs\n",
    "\n",
    "logits = [decoder(state) for decoder in model.decoders]\n",
    "\n",
    "lm_logits, ccg_logits = logits\n",
    "\n",
    "lm_top5 = lm_logits.squeeze().topk(5)\n",
    "for idx, p in zip(lm_top5.indices, lm_top5.values):\n",
    "    print(\"{}:\\t{}\".format(vocab[idx.item()], p))\n",
    "    \n",
    "print(\"---\")\n",
    "ccg_top5 = ccg_logits.squeeze().topk(5)\n",
    "for idx, p in zip(ccg_top5.indices, ccg_top5.values):\n",
    "    print(\"{}:\\t{}\".format(categories[idx.item()], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5d3cf",
   "metadata": {},
   "source": [
    "Ablate individual \"neurons\" in the final layer. (finding highly localized syntactic neurons used for one task but not the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd14c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [2:07:37<00:00, 46.41s/it]\n"
     ]
    }
   ],
   "source": [
    "lm_losses = {}\n",
    "ccg_losses = {}\n",
    "\n",
    "loss_f = nn.NLLLoss()\n",
    "\n",
    "hidden = model.init_hidden(10)\n",
    "with torch.no_grad():\n",
    "    for (input, target_lm, target_ccg) in tqdm.tqdm(test_loader):\n",
    "        if cuda:\n",
    "            input = input.cuda()\n",
    "            target_lm = target_lm.cuda()\n",
    "            target_ccg = target_ccg.cuda()\n",
    "                \n",
    "        input = input.transpose(0,1).contiguous()\n",
    "        target_lm = target_lm.transpose(0,1).contiguous()\n",
    "        target_ccg = target_ccg.transpose(0,1).contiguous()\n",
    "        \n",
    "        state, hidden = model.lstm(input, hidden)\n",
    "        for unit_idx in range(state.shape[-1]):\n",
    "            state_ = state.clone().detach()\n",
    "            state_[:, :, unit_idx] = 0 #ablate\n",
    "            \n",
    "            lm_probs, ccg_probs = [decoder(state_) for decoder in model.decoders]\n",
    "            \n",
    "            lm_losses[unit_idx] = lm_losses.get(unit_idx, 0) + loss_f(lm_probs, target_lm.view(-1)).item()\n",
    "            ccg_losses[unit_idx] = ccg_losses.get(unit_idx, 0) + loss_f(ccg_probs, target_ccg.view(-1)).item()\n",
    "        \n",
    "        # unablated\n",
    "        lm_probs, ccg_probs = [decoder(state) for decoder in model.decoders]\n",
    "            \n",
    "        lm_losses[-1] = lm_losses.get(-1, 0) + loss_f(lm_probs, target_lm.view(-1)).item()\n",
    "        ccg_losses[-1] = ccg_losses.get(-1, 0) + loss_f(ccg_probs, target_ccg.view(-1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed10850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "650",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(ccg_losses, ccg_loss_f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lm_losses\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m----> 9\u001b[0m loss_by_ablation \u001b[38;5;241m=\u001b[39m {i:(lm_losses[i]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader), ccg_losses[i]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lm_losses\u001b[38;5;241m.\u001b[39mitems()))}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ablated_rep.loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m loss_f:\n\u001b[1;32m     12\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(loss_by_ablation, loss_f)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(ccg_losses, ccg_loss_f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lm_losses\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m----> 9\u001b[0m loss_by_ablation \u001b[38;5;241m=\u001b[39m {i:(\u001b[43mlm_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader), ccg_losses[i]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lm_losses\u001b[38;5;241m.\u001b[39mitems()))}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ablated_rep.loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m loss_f:\n\u001b[1;32m     12\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(loss_by_ablation, loss_f)\n",
      "\u001b[0;31mKeyError\u001b[0m: 650"
     ]
    }
   ],
   "source": [
    "with open(\"./ablated_rep_lm.loss\", \"wb\") as lm_loss_f:\n",
    "    pickle.dump(lm_losses, lm_loss_f)\n",
    "    \n",
    "with open(\"./ablated_rep_ccg.loss\", \"wb\") as ccg_loss_f:\n",
    "    pickle.dump(ccg_losses, ccg_loss_f)\n",
    "    \n",
    "print(len(lm_losses.items()))\n",
    "\n",
    "loss_by_ablation = {i:(lm_losses[i]/len(test_loader), ccg_losses[i]/len(test_loader)) for i in range(len(lm_losses.items()))}\n",
    "\n",
    "with open(\"./ablated_rep.loss\", \"wb\") as loss_f:\n",
    "    pickle.dump(loss_by_ablation, loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b3151be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: (4.920966253858624, 1.2028521180152894), 0: (4.92141254309452, 1.2025727788607279), 1: (4.920021029674646, 1.1998506853074737), 2: (4.920931123964714, 1.2011630336443584), 3: (4.920268097790805, 1.2018451770146688), 4: (4.921880266883156, 1.200346716967496), 5: (4.921704280737675, 1.2013453443845112), 6: (4.921463424509222, 1.2015910350915158), 7: (4.921231868050315, 1.2020735614227527), 8: (4.920827848261053, 1.1993766112761064), 9: (4.922034499139497, 1.2021556124542698), 10: (4.920778635776404, 1.201355913552371), 11: (4.921153397993608, 1.2022711865829698), 12: (4.922376184752493, 1.2024511969450749), 13: (4.921757426406398, 1.2023827509446579), 14: (4.921260865529378, 1.2019453207651773), 15: (4.922083253571482, 1.2025107661883037), 16: (4.9258659955227015, 1.2001202016165762), 17: (4.921839412053426, 1.2033789587743355), 18: (4.921325328133323, 1.202247925238176), 19: (4.920799788561735, 1.197259164940227), 20: (4.920733034249508, 1.2018260988322171), 21: (4.921380095048384, 1.2025134422562338), 22: (4.921669798186331, 1.2014259114409938), 23: (4.920946400093309, 1.2009498484206922), 24: (4.921531821742202, 1.202533456773469), 25: (4.9212157003807295, 1.2021550651752588), 26: (4.920958692377264, 1.2018693447113038), 27: (4.920706418066314, 1.2021668795383338), 28: (4.9213750998179115, 1.2010560284961354), 29: (4.921031521305894, 1.2016158494082365), 30: (4.920782483707775, 1.201319229964054), 31: (4.920604253537727, 1.2016042868296306), 32: (4.92126514116923, 1.2014697342207938), 33: (4.921058227076675, 1.2028725385665893), 34: (4.9210358316248115, 1.2028762221336364), 35: (4.921126946535978, 1.2024690357121555), 36: (4.921115605036418, 1.202479322390123), 37: (4.921071077115608, 1.2018772125244142), 38: (4.921289425185233, 1.2025492639252633), 39: (4.92143529400681, 1.2013187130292258), 40: (4.920707760435162, 1.2011086164098796), 41: (4.921068373593417, 1.201465581041394), 42: (4.920728501406583, 1.1971119515823596), 43: (4.92012311184045, 1.197044720071735), 44: (4.9207195281982425, 1.2013259223013213), 45: (4.921657858472882, 1.2008558999408374), 46: (4.921483291279186, 1.2019749887061841), 47: (4.92142605637059, 1.2020038904565753), 48: (4.920976528976903, 1.2018715706738559), 49: (4.921117346214525, 1.202536099246054), 50: (4.921510866916541, 1.2020839669487693), 51: (4.921597483663848, 1.2012702616778288), 52: (4.920935841762659, 1.2020264369068723), 53: (4.920996177557743, 1.1997598167621728), 54: (4.920882544373021, 1.2021298571066423), 55: (4.921074037840872, 1.2027635726061734), 56: (4.921367166981552, 1.2028341177738073), 57: (4.921239403522375, 1.2018526980371187), 58: (4.920792055130005, 1.2025522900350165), 59: (4.920077055150812, 1.1998682484482275), 60: (4.921060102636164, 1.1990254373261422), 61: (4.920785395304362, 1.202152092890306), 62: (4.921239929488211, 1.201810362483516), 63: (4.92135544979211, 1.2016969529065218), 64: (4.921068028247718, 1.2023877761580728), 65: (4.921104579983336, 1.2024788011204113), 66: (4.920834701711481, 1.2016471938653426), 67: (4.921947826038707, 1.2013781634244052), 68: (4.922067934094053, 1.2026876084732288), 69: (4.921461910190004, 1.1974631114439531), 70: (4.921706283453739, 1.2014717669198007), 71: (4.921556527686842, 1.201707894151861), 72: (4.921276575146299, 1.2007526321844622), 73: (4.920745611190796, 1.2018168763680892), 74: (4.9215118538249625, 1.2023967562299787), 75: (4.920608354337288, 1.201783223585649), 76: (4.921072461388328, 1.2024774786197778), 77: (4.920298204999981, 1.2007924766251534), 78: (4.92115013960636, 1.2020644657539599), 79: (4.920673886212436, 1.2027351000092246), 80: (4.921620910817927, 1.2005823250972862), 81: (4.9220049684697935, 1.2020120548479485), 82: (4.921040464170051, 1.2021687641288294), 83: (4.921295074983076, 1.2020717653361235), 84: (4.920852867762248, 1.198058273936763), 85: (4.921088059743245, 1.2014067801562223), 86: (4.921653523589626, 1.2012095216548804), 87: (4.9215761415886154, 1.2016521945144192), 88: (4.921338095809475, 1.2020687233317982), 89: (4.921098277063081, 1.2005470434824626), 90: (4.9212413224306975, 1.2015887303785844), 91: (4.921252096060551, 1.2018440647558732), 92: (4.920677469715928, 1.2025331735610962), 93: (4.921016948873347, 1.201684473138867), 94: (4.92173947131995, 1.2001189892942254), 95: (4.921754669420647, 1.1956042755733838), 96: (4.920791696779656, 1.2004188201644204), 97: (4.922227856607148, 1.2019700451330706), 98: (4.921553818384806, 1.201669821955941), 99: (4.9204796444286, 1.202150002754096), 100: (4.921311099601515, 1.1999544284560464), 101: (4.920937026630749, 1.2014046817114858), 102: (4.921754496025317, 1.202774445215861), 103: (4.921343593886404, 1.2013181906757933), 104: (4.922149481917873, 1.2015172911412788), 105: (4.921353449965968, 1.2021781701030154), 106: (4.921349104967985, 1.20140781547084), 107: (4.9204214370611945, 1.2017976746414647), 108: (4.922461853605328, 1.201530020164721), 109: (4.921198094974865, 1.2022634842179039), 110: (4.921312495433923, 1.201143585913109), 111: (4.920989079908892, 1.201844652855035), 112: (4.920765240987142, 1.20095664804632), 113: (4.921182370908332, 1.201899106574781), 114: (4.921304536588265, 1.202222468997493), 115: (4.9215885090105465, 1.2016049385070802), 116: (4.921327050526937, 1.201223489371213), 117: (4.921497834812511, 1.2022344249667543), 118: (4.921010669072469, 1.2018050833181901), 119: (4.921889805071282, 1.2022778070334232), 120: (4.920322012178826, 1.2011215856581023), 121: (4.920489461494214, 1.2014381130536398), 122: (4.921014989506115, 1.201618999784643), 123: (4.922772696523955, 1.2026173288171942), 124: (4.920840740203857, 1.2027752543940688), 125: (4.920893592545481, 1.2020545905286615), 126: (4.921465879498106, 1.200376779382879), 127: (4.921273118799383, 1.2021518728949807), 128: (4.921309690764456, 1.2020112019596678), 129: (4.921582853432858, 1.2023570591753179), 130: (4.920921084375093, 1.2018588297294848), 131: (4.920692881670865, 1.2023311575253806), 132: (4.92147166223237, 1.2007216901490183), 133: (4.9217972582036795, 1.2013393304564737), 134: (4.9218047488819465, 1.2019596591140285), 135: (4.9212883992628615, 1.2026003432996346), 136: (4.921220117626768, 1.1995370272434118), 137: (4.9218406301556215, 1.1966789267279885), 138: (4.9209713487914115, 1.2020829692031398), 139: (4.922812347701101, 1.1970410206101156), 140: (4.921690646084872, 1.2025681040503762), 141: (4.922149133682251, 1.202764748804497), 142: (4.921419392210065, 1.202271904367389), 143: (4.922509544545954, 1.202737882643035), 144: (4.921628553217108, 1.2020189346689167), 145: (4.921950733300411, 1.2019269809578403), 146: (4.9211195309956866, 1.1987876653671266), 147: (4.921959748412624, 1.201452296069174), 148: (4.920724179527976, 1.2000402367476262), 149: (4.921031709150834, 1.2027929508324826), 150: (4.921067843292699, 1.2018535191362554), 151: (4.920990877440482, 1.200720771153768), 152: (4.92107576601433, 1.1999543078017958), 153: (4.920730723756733, 1.2017855431094313), 154: (4.920982394073949, 1.2013248844580218), 155: (4.920308305277969, 1.2019626368175853), 156: (4.921125135999738, 1.2016973083669489), 157: (4.920724871664336, 1.201797472708153), 158: (4.921326089627815, 1.2018637707739166), 159: (4.921504477298621, 1.201197258270148), 160: (4.921168958779537, 1.201586656859427), 161: (4.920804194248084, 1.1988590919610225), 162: (4.919970190163815, 1.19802130280119), 163: (4.921553760586363, 1.2006354068264817), 164: (4.921224676478992, 1.2023746642199429), 165: (4.921508731264057, 1.2022717139937662), 166: (4.9209473422079375, 1.2025947751420918), 167: (4.920619402509747, 1.2016601981538715), 168: (4.921501626390399, 1.2022992347225998), 169: (4.921122256192294, 1.1997939976778897), 170: (4.922209642872666, 1.2022003809611002), 171: (4.924694490432739, 1.2020093430172314), 172: (4.921291115789702, 1.201898901390307), 173: (4.921746574748646, 1.201354613087394), 174: (4.921042329614813, 1.2022524002826576), 175: (4.92096286686984, 1.2025410576300188), 176: (4.921152208790635, 1.2026482925270543), 177: (4.921153991872614, 1.2009443525112036), 178: (4.920895238356157, 1.199702426881501), 179: (4.921194109772191, 1.200999865026185), 180: (4.920786788246849, 1.2015915321581292), 181: (4.921308335390958, 1.2016151012796343), 182: (4.921431399836685, 1.202174716646021), 183: (4.920773029327393, 1.200876697626981), 184: (4.921204782254768, 1.202311258243792), 185: (4.920559949585885, 1.2015995433836273), 186: (4.921690433675592, 1.200497475898627), 187: (4.920945810549187, 1.2023054415529424), 188: (4.9211294679930715, 1.201691227609461), 189: (4.920856952667236, 1.200898890061812), 190: (4.922207475431038, 1.202185041254217), 191: (4.9202713764075074, 1.198033873240153), 192: (4.9212590477683325, 1.201710615013585), 193: (4.9210423931931, 1.201888550050331), 194: (4.920872431090384, 1.20252677563465), 195: (4.920977420517892, 1.201877320174015), 196: (4.921044651667277, 1.201487604415778), 197: (4.921135197263776, 1.2011677048423073), 198: (4.920468271139896, 1.2019478396935896), 199: (4.921031551650076, 1.2000131473396765), 200: (4.921462819070527, 1.2025152690482863), 201: (4.921332076101592, 1.202202760812008), 202: (4.920859651854544, 1.201483425949559), 203: (4.920864277174979, 1.2013390598875104), 204: (4.92120672717239, 1.1996346964980618), 205: (4.920475842735984, 1.200509663061662), 206: (4.921468598914869, 1.2007028048688715), 207: (4.9210507913069295, 1.2011086366393349), 208: (4.921122980117798, 1.2013575774250609), 209: (4.922433866154064, 1.2025194381222581), 210: (4.921196619669597, 1.2018177473183833), 211: (4.921418689958977, 1.201828936735789), 212: (4.921537892023722, 1.2023015726696362), 213: (4.921176682096539, 1.2014877127878594), 214: (4.92103288823908, 1.2029438723217358), 215: (4.921611339395697, 1.2020741256800564), 216: (4.921332138234919, 1.2015451113382976), 217: (4.9212759697076045, 1.2018180095788205), 218: (4.920687159624967, 1.1988960457570625), 219: (4.9208983088984635, 1.2013197259469466), 220: (4.921692602562182, 1.2014763662309358), 221: (4.921400096199729, 1.2027199774077444), 222: (4.921027673374523, 1.2010785738627117), 223: (4.920663626988729, 1.2005132061062438), 224: (4.921154413801251, 1.2021304029406923), 225: (4.921282641092936, 1.202001901106401), 226: (4.921177290425156, 1.2010999437534449), 227: (4.921071169593118, 1.2016884420857286), 228: (4.921942440668742, 1.2017547264243618), 229: (4.921056857253566, 1.2002618869145711), 230: (4.920840036507808, 1.2020816120234403), 231: (4.9216426517024185, 1.202537594058297), 232: (4.921378495476462, 1.1997941822716685), 233: (4.921522062475031, 1.2025525038892573), 234: (4.921594794591268, 1.2018464612238335), 235: (4.922530301411947, 1.2021330002582435), 236: (4.920323384891857, 1.2015989057945482), 237: (4.9211441487976995, 1.1996181509711525), 238: (4.921466195944584, 1.202762073097807), 239: (4.921482353499441, 1.2020888801777), 240: (4.921332756678264, 1.2018949143814317), 241: (4.9217851450949, 1.2026722373384418), 242: (4.921218925533872, 1.2016734451958628), 243: (4.921546321926695, 1.2028552170955773), 244: (4.92153703660676, 1.1997392788077845), 245: (4.921404072732637, 1.2022370547959298), 246: (4.920995924689553, 1.2021637880440914), 247: (4.9236253984046705, 1.2023061376629454), 248: (4.921452369111957, 1.2001031200091044), 249: (4.920675271930117, 1.2006083824417808), 250: (4.921326127196803, 1.2023051420847575), 251: (4.921225836782744, 1.200752906365828), 252: (4.921096310471043, 1.1985751578302095), 253: (4.921859783114809, 1.2011805758331762), 254: (4.921023487322258, 1.2014540834860368), 255: (4.921402142264626, 1.2023281173272566), 256: (4.920430300452493, 1.2027199882449526), 257: (4.920669145295114, 1.2014327464681682), 258: (4.921617486260154, 1.202010660460501), 259: (4.9213476701216265, 1.2022255471258452), 260: (4.921015878157182, 1.2020294911933669), 261: (4.9213419437408445, 1.202427153153853), 262: (4.92144036582022, 1.2024999221165975), 263: (4.921302824309378, 1.2025573842453234), 264: (4.9203491080891, 1.201344283060594), 265: (4.921607264605435, 1.202671173847083), 266: (4.921249516805013, 1.2016294678052266), 267: (4.921428839365642, 1.202613397439321), 268: (4.920452600536924, 1.1941283020106228), 269: (4.921084077430494, 1.2019062566034722), 270: (4.92096375985579, 1.201851480296164), 271: (4.921172692558982, 1.2021929795091801), 272: (4.921390591245709, 1.2018217516667915), 273: (4.922056173555779, 1.2019685163642422), 274: (4.920911339557532, 1.2016868782765937), 275: (4.921245761351152, 1.2022262352885622), 276: (4.920911877083056, 1.2021193858348962), 277: (4.921380576220426, 1.2015147610144181), 278: (4.920802343252934, 1.2007302183093447), 279: (4.920910372878566, 1.2009568492571512), 280: (4.920597270040801, 1.201623219432253), 281: (4.92142001209837, 1.201245216889815), 282: (4.92164475701072, 1.2023910638057824), 283: (4.921831921375159, 1.2015814521095969), 284: (4.921217562935569, 1.2012156439549995), 285: (4.920489978790283, 1.202235526749582), 286: (4.921237415255923, 1.200753557320797), 287: (4.920817525459058, 1.2016957355268074), 288: (4.920914790124604, 1.2010648463711595), 289: (4.921606701070612, 1.2018580931605716), 290: (4.92079754598213, 1.2024917656725103), 291: (4.921025279074004, 1.2007781093770808), 292: (4.921170665278579, 1.2021999503626968), 293: (4.921143286155932, 1.2018886024301703), 294: (4.921005348725752, 1.2023245681415904), 295: (4.921488841374715, 1.2019089362838051), 296: (4.921119226108898, 1.2022210023619913), 297: (4.921270530874079, 1.2014270554889332), 298: (4.921328348101992, 1.2026428764516657), 299: (4.921757448080815, 1.2017461852593856), 300: (4.921280827666774, 1.2021451776677912), 301: (4.9212687550169045, 1.2021983833023997), 302: (4.921240560936205, 1.2014613751209144), 303: (4.92184574098298, 1.2010098435662009), 304: (4.921126858393351, 1.2027728376966533), 305: (4.921290498791319, 1.2025722453088472), 306: (4.920307306809859, 1.2011309096307465), 307: (4.920887334419019, 1.2018967599579782), 308: (4.921201250769875, 1.2029703729080432), 309: (4.921176216819069, 1.2005147258440654), 310: (4.9203396074699635, 1.201260055917682), 311: (4.921086317120176, 1.202774064468615), 312: (4.9212848432136305, 1.2028000051325018), 313: (4.920873735890244, 1.201728270631848), 314: (4.920930885546135, 1.2008320436333164), 315: (4.921490823861324, 1.2026012081088442), 316: (4.920885437185114, 1.2019824862480164), 317: (4.92101560939442, 1.2015252915295687), 318: (4.920902614882498, 1.2020340800285338), 319: (4.92085859414303, 1.1938568382552175), 320: (4.920470887964422, 1.201890014518391), 321: (4.921513269886826, 1.2019996007283529), 322: (4.921670412294793, 1.2015351649486656), 323: (4.921001325954091, 1.201168708006541), 324: (4.921750158252138, 1.2026093891172698), 325: (4.921445819103357, 1.2014436754313382), 326: (4.921893366900357, 1.2011352719682635), 327: (4.920725961164995, 1.2011063048333832), 328: (4.9212868430397725, 1.2017958655501857), 329: (4.92116162704699, 1.2022527358748696), 330: (4.921110119964137, 1.2010640809030244), 331: (4.92157778450937, 1.2009589957468438), 332: (4.920648675976378, 1.2007374492558567), 333: (4.921164485180017, 1.2009302442724055), 334: (4.920796079346628, 1.2025264516021266), 335: (4.921573501644713, 1.2022226196346861), 336: (4.9209328290187955, 1.2021690505923648), 337: (4.921360193599354, 1.2015175382296244), 338: (4.922131539836075, 1.2003442348855915), 339: (4.920954939813325, 1.2016668919361), 340: (4.921424349871549, 1.2024361057714983), 341: (4.921242101264722, 1.2006193085150285), 342: (4.9217299851504235, 1.201860197023912), 343: (4.921829418702559, 1.2027287349556433), 344: (4.921461638537321, 1.202185495333238), 345: (4.921403123393203, 1.2016936107115312), 346: (4.921079444885254, 1.2024410359787219), 347: (4.921090997349133, 1.2015993678208554), 348: (4.920996535908092, 1.2007542151393313), 349: (4.920948199069861, 1.2018898425680218), 350: (4.921201308568318, 1.2015763102155743), 351: (4.92132914572051, 1.2011287313519101), 352: (4.921684143759988, 1.2005515116633791), 353: (4.921228716590187, 1.2004534475731128), 354: (4.922136081348766, 1.202685428388191), 355: (4.9212021856596975, 1.2019186807401252), 356: (4.920400337739424, 1.2014210654027535), 357: (4.921456698215369, 1.2021764946706368), 358: (4.921122714244958, 1.2013391288844022), 359: (4.921817312818585, 1.2015063686804337), 360: (4.920930131276449, 1.2014936804771423), 361: (4.921262303265658, 1.201504683494568), 362: (4.920573889125477, 1.2020824739427278), 363: (4.921146583557129, 1.201504927331751), 364: (4.920785820122921, 1.2022502476518804), 365: (4.91928946466157, 1.197259862133951), 366: (4.921237990350434, 1.200808318456014), 367: (4.921672682328658, 1.201557650349357), 368: (4.92119203191815, 1.2011793154658694), 369: (4.921415915633693, 1.2023656913728424), 370: (4.9210471297755385, 1.2020611596829964), 371: (4.921680502458052, 1.2017972978678617), 372: (4.9212549859827215, 1.1987657232718034), 373: (4.9217311165549535, 1.2024437514218418), 374: (4.9204822179042935, 1.201472822825114), 375: (4.9222421862862324, 1.2020387523102039), 376: (4.919728176521533, 1.200351930025852), 377: (4.920636424151334, 1.1954446839563775), 378: (4.92281700481068, 1.202408854527907), 379: (4.921161291816018, 1.201781333215309), 380: (4.920927046284531, 1.201583536827203), 381: (4.921012956445868, 1.202212102846666), 382: (4.92101257353118, 1.2018179586439421), 383: (4.921303893580581, 1.202113435123906), 384: (4.920861197962905, 1.2016671372182441), 385: (4.921085857622551, 1.2018810044635426), 386: (4.9209647727735115, 1.2024028377099472), 387: (4.921058045011578, 1.2000937841155312), 388: (4.921583172769258, 1.2008606354395548), 389: (4.920945254239169, 1.20015027848157), 390: (4.920862953590624, 1.2000932603171377), 391: (4.92084528316151, 1.202506122083375), 392: (4.920313916061864, 1.1992828853202588), 393: (4.921132162845496, 1.2006718194845951), 394: (4.920782018430305, 1.1975700154449), 395: (4.92095960559267, 1.202271410190698), 396: (4.921190883174087, 1.2018778244654338), 397: (4.920566742347948, 1.2022984999598878), 398: (4.921144622744936, 1.2006091981223135), 399: (4.921042029062907, 1.2023006883534517), 400: (4.9214213212331135, 1.2017272436257564), 401: (4.921285132205847, 1.1998847824154477), 402: (4.920736333095666, 1.2008158441745873), 403: (4.921708518808538, 1.2016859137650693), 404: (4.9207989172502, 1.200556713884527), 405: (4.923027437383478, 1.199145910956643), 406: (4.9211096720262, 1.2004607778606993), 407: (4.921241182269472, 1.2015453049630829), 408: (4.920873092882561, 1.20285861528281), 409: (4.9210561362179845, 1.2018585992581916), 410: (4.9213782498330785, 1.2027955225019744), 411: (4.920547605283333, 1.1997855815020475), 412: (4.921051149657278, 1.2014505100972726), 413: (4.920898079149651, 1.1989150856480455), 414: (4.9214687390760945, 1.2023683717756561), 415: (4.921549219073671, 1.2014662753451955), 416: (4.9213694355704565, 1.2028326720902414), 417: (4.921319250626998, 1.2027098655700683), 418: (4.922115127968065, 1.1996271299593377), 419: (4.92165239796494, 1.201579541871042), 420: (4.921183129512903, 1.201559061353857), 421: (4.9223846912384035, 1.2020232962839532), 422: (4.920992023294622, 1.1998461499358668), 423: (4.920334431619355, 1.2008021463047374), 424: (4.921217525366581, 1.1972522388805042), 425: (4.9213274088772865, 1.2027362863222757), 426: (4.922308953603109, 1.202275856697198), 427: (4.920939173842921, 1.2005762241103433), 428: (4.921662386980924, 1.2017223571286058), 429: (4.920449897014733, 1.196226875709765), 430: (4.920716456210974, 1.2012455824649695), 431: (4.921335649490357, 1.2020705931114428), 432: (4.921453143611099, 1.202378428704811), 433: (4.922457534616644, 1.2021945093617294), 434: (4.921375429269039, 1.202718683083852), 435: (4.921269133596709, 1.2021196018565785), 436: (4.921539032098019, 1.2016519131082477), 437: (4.9206549153183445, 1.2000231757308497), 438: (4.921304913723108, 1.202090682405414), 439: (4.920968996394764, 1.202679737408956), 440: (4.9215100273941506, 1.201923884044994), 441: (4.921378702105898, 1.2013911630168106), 442: (4.921761112502127, 1.2009278344385552), 443: (4.921120727423466, 1.2017606789415534), 444: (4.921376809206876, 1.2019843527764984), 445: (4.921238579894557, 1.2021240371646302), 446: (4.921572685241699, 1.201535255981214), 447: (4.920744839581576, 1.2023010896913933), 448: (4.921611567699548, 1.2011748797965773), 449: (4.9203708070697205, 1.202263565496965), 450: (4.921331616603967, 1.2021229169585488), 451: (4.9212929942391135, 1.202166784170902), 452: (4.922505150419293, 1.2023051251064647), 453: (4.921165378165967, 1.201378095511234), 454: (4.920801551414258, 1.2013901652711811), 455: (4.921503225962321, 1.2025925827748847), 456: (4.921085229064479, 1.202391744743694), 457: (4.920949487975149, 1.2020008181080675), 458: (4.922077288772121, 1.202388014937892), 459: (4.921116467678186, 1.201005659681378), 460: (4.921062914530436, 1.2015909473101298), 461: (4.922114249431726, 1.20290560613979), 462: (4.920905189803152, 1.2023253556453821), 463: (4.922157968174328, 1.2000059015823132), 464: (4.921752439845692, 1.2020313169016983), 465: (4.920703802686749, 1.2020962722373731), 466: (4.921360236948186, 1.2006866014364994), 467: (4.922017733256022, 1.2027666463996425), 468: (4.920768262400772, 1.2019135569081163), 469: (4.920927426309296, 1.201417321508581), 470: (4.920929774371061, 1.2013819362177993), 471: (4.920763017192032, 1.1970881281477033), 472: (4.920990191083966, 1.2022003863797044), 473: (4.921031165845466, 1.2015064803036777), 474: (4.92094216635733, 1.2014211495717366), 475: (4.920381751204982, 1.202837514516079), 476: (4.920694151791659, 1.2017539548151421), 477: (4.920524741664077, 1.1995692054430644), 478: (4.9210668144804055, 1.2020919015913298), 479: (4.921214366681648, 1.2022428964123582), 480: (4.921809329408588, 1.20108659881534), 481: (4.921894924568408, 1.2021910251993122), 482: (4.918944851557414, 1.196452238704219), 483: (4.9204822121244485, 1.2021639220642322), 484: (4.921549326000791, 1.2015524221189093), 485: (4.921531417153099, 1.2016130342628017), 486: (4.92090292410417, 1.2002429048220316), 487: (4.9207758137674045, 1.2015589699600682), 488: (4.921076013102676, 1.2013272538329616), 489: (4.920702718965935, 1.200731778867317), 490: (4.921432180115671, 1.2020927375013177), 491: (4.921030723687374, 1.2021648934393219), 492: (4.920872009161747, 1.2014492002400485), 493: (4.920512589541349, 1.2003150986902642), 494: (4.921164901328809, 1.2013728199583111), 495: (4.921365219174009, 1.2025475548975395), 496: (4.921240536371867, 1.2029843500166228), 497: (4.921290862921513, 1.2010602145484), 498: (4.920745267290058, 1.2020808097087976), 499: (4.920763595176465, 1.2024385719588309), 500: (4.920361383033521, 1.2016734188253229), 501: (4.921053879188769, 1.2023240064129685), 502: (4.921237246195475, 1.2023009964914033), 503: (4.921125274716002, 1.2013141335863056), 504: (4.9215931285511365, 1.2019797527428828), 505: (4.921176780353893, 1.2017012227665294), 506: (4.9205648133249, 1.200323659362215), 507: (4.9207190137920955, 1.201927567250801), 508: (4.921102330178925, 1.2027004108284458), 509: (4.920935828758009, 1.2010376424500437), 510: (4.920742718378703, 1.1970041368946884), 511: (4.92079459103671, 1.2022677273461313), 512: (4.921822258920381, 1.201231479283535), 513: (4.921763008291071, 1.2016061800898927), 514: (4.921343826525139, 1.2010981942668106), 515: (4.920653415448738, 1.2013981490424186), 516: (4.9210922689148875, 1.2004744294917944), 517: (4.920561904618235, 1.2015380732940906), 518: (4.921757643150561, 1.2016937786882573), 519: (4.921069919701779, 1.2020852417656869), 520: (4.921321052493471, 1.19988924481652), 521: (4.922048698772084, 1.1998665910778623), 522: (4.921103663878007, 1.2014416333400841), 523: (4.921429586410523, 1.202714826482715), 524: (4.921464035727761, 1.2023891300866099), 525: (4.922352676680593, 1.2019382913907368), 526: (4.921459099740693, 1.2021087592298334), 527: (4.921624065167976, 1.1995175661462727), 528: (4.920537190003828, 1.2020708401997884), 529: (4.921491222670584, 1.2010462912646207), 530: (4.92081289580374, 1.201904246301362), 531: (4.921316026918816, 1.2016613696560716), 532: (4.921754902059382, 1.2021909204396335), 533: (4.9216143665891705, 1.2025938763762964), 534: (4.920457996021618, 1.2019343321973628), 535: (4.921468064279267, 1.2016361977114822), 536: (4.921470093004631, 1.2015999512238935), 537: (4.9213461355729535, 1.202300454269756), 538: (4.920984064448963, 1.2025871374390342), 539: (4.922214941544966, 1.2025783094492826), 540: (4.92058703104655, 1.1972959536494632), 541: (4.921228624112678, 1.2012538180206762), 542: (4.921396706321023, 1.2003336527130821), 543: (4.921061966635964, 1.2016063943053736), 544: (4.921735080083211, 1.2012194875514868), 545: (4.921603465802741, 1.2021174174366576), 546: (4.921316461852102, 1.200696059068044), 547: (4.922011410106312, 1.2006512569658685), 548: (4.921060078071825, 1.2018064365242467), 549: (4.920670357617465, 1.1977657108595876), 550: (4.920434196067579, 1.1969970269636674), 551: (4.9212469216549035, 1.2018752975897355), 552: (4.921357715491093, 1.1996159275372824), 553: (4.921115983616222, 1.1993278474518747), 554: (4.921125214027636, 1.2018304149309793), 555: (4.921404282251994, 1.2023874748836865), 556: (4.921622169379032, 1.2020450393358866), 557: (4.921957151817553, 1.2023576227101413), 558: (4.921181171590631, 1.2018741950844274), 559: (4.920909917715824, 1.2018538735129616), 560: (4.921511488249807, 1.2014145869197268), 561: (4.92105955210599, 1.2015467130776607), 562: (4.920491562467633, 1.2018678412292942), 563: (4.921766311472113, 1.2016633926015912), 564: (4.9217721043211045, 1.199560331214558), 565: (4.921665165641091, 1.2026450485894173), 566: (4.92103037111687, 1.2024561762809753), 567: (4.921829476501003, 1.2001890742417538), 568: (4.921990271770593, 1.202299429431106), 569: (4.922213131008726, 1.2026985078146963), 570: (4.921712264147672, 1.202771662943291), 571: (4.921435960133871, 1.2023514653697158), 572: (4.921155740275528, 1.202253937360012), 573: (4.921204120462591, 1.2020782546563582), 574: (4.921054101712776, 1.2024688424486103), 575: (4.921608549175841, 1.2027154958609378), 576: (4.920824082692464, 1.2024375409790964), 577: (4.922986229983243, 1.2026253552147836), 578: (4.921298253897465, 1.2016454479911112), 579: (4.92130527640834, 1.2025610739534551), 580: (4.920910224047574, 1.1995080749193827), 581: (4.920872282259392, 1.2001107660206882), 582: (4.92679302331173, 1.2019272341872707), 583: (4.92143787615227, 1.2027823766072592), 584: (4.92017571998365, 1.1993778662248091), 585: (4.920685801361547, 1.2002530907139635), 586: (4.921210657466542, 1.2021282398339475), 587: (4.920939711368445, 1.2029409643375513), 588: (4.921678361025724, 1.2022850954171382), 589: (4.922028807437781, 1.2013642134088458), 590: (4.92088178432349, 1.1997140057159192), 591: (4.921493523048632, 1.2008210864934055), 592: (4.920685669870087, 1.2008096492651736), 593: (4.920692998712713, 1.20127557479974), 594: (4.921575368534435, 1.2018202983971797), 595: (4.9212887431636005, 1.202014278643059), 596: (4.921244382858276, 1.2028671900431316), 597: (4.9220244176460035, 1.2018361821319117), 598: (4.922024539022734, 1.2006357832388443), 599: (4.920763242605961, 1.2011827656717011), 600: (4.921537600141583, 1.202152312524391), 601: (4.920631550297593, 1.1964425227858804), 602: (4.920621964425751, 1.2009182138876482), 603: (4.920866456176295, 1.2019239382310347), 604: (4.921037835785837, 1.2016204512480533), 605: (4.921278794606526, 1.200565100077427), 606: (4.921317538348111, 1.2025992505478136), 607: (4.920684891036062, 1.2018086996945467), 608: (4.920951914064812, 1.1964775742906513), 609: (4.9217726577412, 1.2013759071176702), 610: (4.921294205116503, 1.20214548219334), 611: (4.9225068858175565, 1.2013666918783477), 612: (4.9206711306716455, 1.199898777586041), 613: (4.920798037268899, 1.2012269525816945), 614: (4.92293428218726, 1.2022063186674408), 615: (4.921131082014604, 1.1992167042963433), 616: (4.921471851522273, 1.2026013280406143), 617: (4.921488851489443, 1.2027166789228265), 618: (4.921223971337983, 1.2020639643524633), 619: (4.92094082254352, 1.202009375890096), 620: (4.921095902992017, 1.201385001702742), 621: (4.921315947445956, 1.2015824747808053), 622: (4.921452156702677, 1.2002053625655897), 623: (4.921583817221902, 1.2016074039719322), 624: (4.920938416683312, 1.2010271086837305), 625: (4.921421971465602, 1.202457228935126), 626: (4.9209839676365705, 1.2020305980335582), 627: (4.9210195729226776, 1.2017258253964511), 628: (4.920164891445276, 1.2015573866439588), 629: (4.920952026771777, 1.2027780269131516), 630: (4.921495004133745, 1.2023508368116436), 631: (4.9212185310594965, 1.200503793629733), 632: (4.921645340774998, 1.2006458127137387), 633: (4.921115629600756, 1.2023193698940855), 634: (4.920817366513339, 1.2010968724886577), 635: (4.920660940806071, 1.2013984196113818), 636: (4.921535405245694, 1.2012110724593654), 637: (4.920951983422944, 1.200886403791832), 638: (4.921305961319894, 1.2014331405813043), 639: (4.921414170120702, 1.2025393399325284), 640: (4.921109903219975, 1.2027149948206814), 641: (4.921381636821862, 1.201271609465281), 642: (4.921266678607825, 1.2021272301673889), 643: (4.921419561270511, 1.2013744560155002), 644: (4.921398683027788, 1.2009010155995687), 645: (4.920634534142234, 1.2022491729620732), 646: (4.9216264955925215, 1.2010450493205678), 647: (4.920840156439579, 1.202627576119972), 648: (4.922732581514301, 1.201998575889703), 649: (4.9206944118846545, 1.2019104426557368)}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ablated_rep_lm.loss\", \"rb\") as lm_loss_f:\n",
    "    lm_losses = pickle.load(lm_loss_f)\n",
    "    \n",
    "with open(\"./ablated_rep_ccg.loss\", \"rb\") as ccg_loss_f:\n",
    "    ccg_losses = pickle.load(ccg_loss_f)\n",
    "\n",
    "loss_by_ablation = {(i-1):(lm_losses[i-1]/len(test_loader), ccg_losses[i-1]/len(test_loader)) for i in range(len(lm_losses.items()))}\n",
    "\n",
    "print(loss_by_ablation)\n",
    "\n",
    "csv_losses = []\n",
    "\n",
    "for i, (lm_loss, ccg_loss) in loss_by_ablation.items():\n",
    "    csv_losses.append({\"ablated_dim\": i,\n",
    "                       \"lm_loss\": lm_loss,\n",
    "                       \"ccg_loss\": ccg_loss})\n",
    "    \n",
    "import csv\n",
    "\n",
    "with open(\"ablated_rep.csv\", \"w\") as csv_out_f:\n",
    "    writer = csv.DictWriter(csv_out_f, fieldnames = csv_losses[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c9565",
   "metadata": {},
   "source": [
    "Use a non-iterated version of iNLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b581ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lmccg_path = \"../CCGMultitask/models/augment/augment_.50_0_sgd_continue\"\n",
    "model_lmonly_path = \"../CCGMultitask/models/augment/augment_1.00_0_sgd_continue\"\n",
    "model_ccgprobe_path = \"./models/augment_1.00_0_sgd_continue_ccgfrozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c382ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    with open(model_path + \".w2idx\", \"rb\") as w2idx_f:\n",
    "        w2idx = pickle.load(w2idx_f)\n",
    "    \n",
    "    vocab = {i:w for (w, i) in w2idx.items()}\n",
    "    \n",
    "    with open(model_path + \".c2idx\", \"rb\") as c2idx_f:\n",
    "        c2idx = pickle.load(c2idx_f)\n",
    "    \n",
    "    categories = {i:c for (c, i) in c2idx.items()}\n",
    "    \n",
    "    model = MultiTaskModel(len(w2idx.keys()), 650, 650, [len(w2idx.keys()), len(c2idx.keys())], 2)\n",
    "    model.load_state_dict(torch.load(model_path + \".pt\", map_location = torch.device(\"cpu\")))\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    if cuda: model.cuda()\n",
    "        \n",
    "    return model, w2idx, vocab, c2idx, categories\n",
    "\n",
    "model_lmccg, w2idx_lmccg, vocab_lmccg, c2idx_lmccg, categories_lmccg = load_model(model_lmccg_path)\n",
    "model_lmonly, w2idx_lmonly, vocab_lmonly, c2idx_lmonly, categories_lmonly = load_model(model_lmonly_path)\n",
    "model_ccgprobe, w2idx_ccgprobe, vocab_ccgprobe, c2idx_ccgprobe, categories_ccgprobe = load_model(model_ccgprobe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28446c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_lmccg = joint_tag_lm(\"../CCGMultitask/data/ccg_supertags/ccg.23.common\", \n",
    "                         \"../CCGMultitask/data/ccg_supertags/categories\", 35, w2idx=w2idx_lmccg)\n",
    "test_sampler_lmccg = BatchSampler(test_data_lmccg, 10)\n",
    "test_loader_lmccg = DataLoader(test_data_lmccg, batch_sampler=test_sampler_lmccg)\n",
    "\n",
    "test_data_lmonly = joint_tag_lm(\"../CCGMultitask/data/ccg_supertags/ccg.23.common\", \n",
    "                         \"../CCGMultitask/data/ccg_supertags/categories\", 35, w2idx=w2idx_lmonly)\n",
    "test_sampler_lmonly = BatchSampler(test_data_lmonly, 10)\n",
    "test_loader_lmonly = DataLoader(test_data_lmonly, batch_sampler=test_sampler_lmonly)\n",
    "\n",
    "test_data_ccgprobe = joint_tag_lm(\"../CCGMultitask/data/ccg_supertags/ccg.23.common\", \n",
    "                         \"../CCGMultitask/data/ccg_supertags/categories\", 35, w2idx=w2idx_ccgprobe)\n",
    "test_sampler_ccgprobe = BatchSampler(test_data_ccgprobe, 10)\n",
    "test_loader_ccgprobe = DataLoader(test_data_ccgprobe, batch_sampler=test_sampler_ccgprobe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4915bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3007e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ccg_decoder \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoders[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m W \u001b[38;5;241m=\u001b[39m ccg_decoder\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m basis \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnull_space(W)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "ccg_decoder = model.decoders[1]\n",
    "\n",
    "W = ccg_decoder.linear.weight.detach().cpu().numpy()\n",
    "\n",
    "basis = linalg.null_space(W)\n",
    "\n",
    "P = basis.dot(basis.T)\n",
    "\n",
    "sum(abs(W.dot(P.dot(np.random.rand(650)))) < 1e-5) == 427\n",
    "\n",
    "P = torch.Tensor(P)\n",
    "\n",
    "if cuda: P = P.cuda()\n",
    "\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef93a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_P(weight):\n",
    "    W = weight.detach().cpu().numpy()\n",
    "    basis = linalg.null_space(W)\n",
    "    P = basis.dot(basis.T)\n",
    "\n",
    "    P = torch.Tensor(P)\n",
    "    if cuda: P = P.cuda()\n",
    "        \n",
    "    return P\n",
    "\n",
    "def nsp_losses(model, P, test_loader):\n",
    "    lm_loss = 0\n",
    "    ccg_loss = 0\n",
    "    ccg_correct = 0\n",
    "\n",
    "    nsp_lm_loss = 0\n",
    "    nsp_ccg_loss = 0\n",
    "    nsp_ccg_correct = 0\n",
    "\n",
    "    total_examples = 0\n",
    "    \n",
    "    loss_f = nn.NLLLoss()\n",
    "\n",
    "    hidden = model.init_hidden(10)\n",
    "    with torch.no_grad():\n",
    "        for (input, target_lm, target_ccg) in tqdm.tqdm(test_loader):\n",
    "            if cuda:\n",
    "                input = input.cuda()\n",
    "                target_lm = target_lm.cuda()\n",
    "                target_ccg = target_ccg.cuda()\n",
    "                \n",
    "            input = input.transpose(0,1).contiguous()\n",
    "            target_lm = target_lm.transpose(0,1).contiguous()\n",
    "            target_ccg = target_ccg.transpose(0,1).contiguous()\n",
    "        \n",
    "            state, hidden = model.lstm(input, hidden)\n",
    "            state_ = state.clone().detach()\n",
    "            state_ = torch.matmul(P, state_.view(state_.shape[0], state_.shape[1], state_.shape[2], 1))\n",
    "            state_ = state_.squeeze()\n",
    "\n",
    "            num_examples = len(target_lm.view(-1))\n",
    "            total_examples += num_examples\n",
    "        \n",
    "            nsp_lm_probs, nsp_ccg_probs = [decoder(state_) for decoder in model.decoders]\n",
    "            nsp_lm_loss += num_examples * loss_f(nsp_lm_probs, target_lm.view(-1)).item()\n",
    "            nsp_ccg_loss += num_examples * loss_f(nsp_ccg_probs, target_ccg.view(-1)).item()\n",
    "            nsp_ccg_correct += (nsp_ccg_probs.argmax(dim=1) == target_ccg.view(-1)).sum().item()\n",
    "        \n",
    "            lm_probs, ccg_probs = [decoder(state) for decoder in model.decoders]\n",
    "            lm_loss += num_examples * loss_f(lm_probs, target_lm.view(-1)).item()\n",
    "            ccg_loss += num_examples * loss_f(ccg_probs, target_ccg.view(-1)).item()\n",
    "            ccg_correct += (ccg_probs.argmax(dim=1) == target_ccg.view(-1)).sum().item()\n",
    "        \n",
    "        \n",
    "    lm_loss = lm_loss/total_examples\n",
    "    ccg_loss = ccg_loss/total_examples\n",
    "    ccg_accuracy = ccg_correct/total_examples\n",
    "    \n",
    "    nsp_lm_loss = nsp_lm_loss/total_examples\n",
    "    nsp_ccg_loss = nsp_ccg_loss/total_examples\n",
    "    nsp_ccg_accuracy = nsp_ccg_correct/total_examples\n",
    "    \n",
    "    return lm_loss, ccg_loss, ccg_accuracy, nsp_lm_loss, nsp_ccg_loss, nsp_ccg_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8017dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f6cf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:25<00:00,  6.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:26<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no intervention: lm 4.920966253858624, ccg 1.2028521180152894/0.8448658008658009\n",
      "intervention: lm 7.180473087773178, ccg 6.509100890882087/0.21229437229437229\n",
      "random proj: lm 7.83287026087443, ccg 6.464027540611498/0.21229437229437229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _, rand_lm_loss, rand_ccg_loss, rand_ccg_accuracy = nsp_losses(model_lmccg, get_P(torch.rand((650, 650))).cuda(), test_loader_lmccg)\n",
    "print(\"no intervention: lm {}, ccg {}/{}\\nintervention: lm {}, ccg {}/{}\\nrandom proj: lm {}, ccg {}/{}\".format(\n",
    "    *nsp_losses(model_lmccg, get_P(model_lmccg.decoders[1].linear.weight), test_loader_lmccg), \n",
    "    rand_lm_loss, rand_ccg_loss, rand_ccg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabf9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:25<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no intervention: lm 4.325432572220311, ccg 6.098769101229581/0.002718614718614719\n",
      "intervention: lm 5.966211801586729, ccg 6.056785089319402/0.07601731601731601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"no intervention: lm {}, ccg {}/{}\\nintervention: lm {}, ccg {}/{}\".format(\n",
    "    *nsp_losses(model_lmonly, get_P(model_lmonly.decoders[1].linear.weight), test_loader_lmonly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86666765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:25<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no intervention: lm 4.325432572220311, ccg 0.5187777609536142/0.8429783549783549\n",
      "intervention: lm 7.181911335569439, ccg 4.455898727070202/0.21229437229437229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"no intervention: lm {}, ccg {}/{}\\nintervention: lm {}, ccg {}/{}\".format(\n",
    "    *nsp_losses(model_ccgprobe, get_P(model_ccgprobe.decoders[1].linear.weight), test_loader_ccgprobe)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (gpu)",
   "language": "python",
   "name": "python3gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
